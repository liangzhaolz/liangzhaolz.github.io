---
title: "Intro to brms"
author: "E Chodroff"
date: "15/04/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

The goal for today is to learn how to use the package brms. We'll start off with some examples using the better known package lme4, and learn how to 'translate' the syntax and output between the two types of models.

# Buckeye VOT

The first example will make use of the Buckeye VOT dataset. You'll need to update the path to reflect the dataset's location on your own computer.

```{r message = FALSE}
require(tidyverse)
require(lme4)
require(brms)

buck <- read_csv("~/Dropbox/2019-2020/buckeye_vot.csv")
buck.orig <- buck
```

Let's first take a look at the dataset so we know what we're working with. We're going to be focusing on predicting VOT from various factors like age (young vs old), stop category, and vowel duration as a rough proxy for speaking rate. We'll also want to correct for potential speaker differences.

brms has many advantages over lme4, but it does require substantially more time to run. In light of this, we're going to reduce the size of the dataset for today's exercises.

```{r }
buck <- subset(buck, gender == "f")
buck <- subset(buck, phon %in% c('p', 't', 'k'))
```

## lme4 linear regression examples

Now that we have a slightly smaller dataset, let's start off with a simple mixed-effects linear regression model. We would like to predict VOT from the stop category (buck`$phon`), following vowel duration (buck`$vdur`), and the person's age (buck`$age`). We'd also like to correct for between-speaker differences in overall VOT, so we'll include a random intercept for speaker (buck`$spkr`). We'll first set up the contrasts for the categorical predictors of phon and age.

```{r }
buck$phon <- factor(buck$phon, levels=c('t','k','p'))
contrasts(buck$phon) <- contr.sum(3)

contrasts(buck$phon)

buck$age <- factor(buck$age, levels=c('y','o'))
contrasts(buck$age) <- contr.sum(2)

contrasts(buck$age)
```

Now for the lmer() regression with a simple random effect structure:

```{r }
fit.simple <- lmer(vot ~ phon + vdur + age + (1|spkr), buck)
summary(fit.simple)
```

It converges, yay! But what if we want to include the full random effect structure for speaker? It might be that stop category and speaking rate have unique effects on VOT by speaker -- we should correct for those too:

```{r }
fit.max <- lmer(vot ~ phon + vdur + age + (1 + phon + vdur|spkr), buck)
summary(fit.max)
```

Oh no, what to do? Drop random effects until it converges, or we can switch to brms. I will now note that this is most definitely *not* the only reason you should use brms. And in fact, you might take some flak for doing it, but alas. 

Let's first take a look at how to run brms with the simple model. 

## brms linear regression example 1
```{r eval=F}
fit.simple.brms <- brm(vot ~ phon + vdur + age + (1|spkr), buck)
```

### What are we doing, round 1

Now, we wait. In the meantime, you might be wondering, isn't this a Bayesian model? Where's the prior? What's happening? Well, we're first going to run fast and loose. 

The prior is there! We're using the default... we'll come back to this, but maybe next time. 

*As for what's happening:* the brms package piggybacks on the Stan programming language which can estimate complex model parameters by sampling parameter values and homing in on ones that better account for the data. Recall that the goal of a Bayesian analysis is to *obtain the posterior distribution* of the model given some data and also some prior knowledge about the distributions of the parameters of interest. The analysis will return a list of estimates for the marginal posterior distribution of each parameter under consideration. (It's marginal because several parameters are being estimated at once.)

*So what is the default prior?* The default prior is what's called an improper uniform distribution over real numbers. From what I understand, this distribution puts the same little amount of probability over (most) real numbers. This is the least informative prior you can use in Bayesian analysis, which can be good if you'd like to stay agnostic, but could also bad as you might be misleading the model. If you know that VOTs range from 0 to maximally 250 ms, why are you letting your model even consider parameter values (e.g., beta estimates) that would take the VOT value outside this range?

```{r }
#summary(fit.simple.brms)
```

### Understanding the output

What does the output look like? For the fast and loose implementation of brms, we can interpret this in a parallel fashion to lme4 (but I emphasize the fast and loose aspect). 

We're typically most interested in the population-level effects: this parallels your fixed effects table in lme4. The three columns that are most critical for a standard analysis are 1) the estimate, 2) the l-95% CI and 3) the u-95% CI. The first should look familiar: this is the estimated beta coeffecient for each parameter. If you compare these values to the lme4 output, they probably look pretty similar, especially since we had an uninformative prior. 

*Now for the significance test:* there is no significance test. There are no p-values, no t-values. Well, what is it that we're asking when we run a frequentist null hypothesis significance test (NHST)? We want to know how probable the estimated effect is assuming the null hypothesis, where the null hypothesis is that the effect should be 0: 0 means there's *no* effect of this factor on y. Assuming no random effects in our linear regression, we use a t distribution as our null hypothesis probability distribution. The t distribution is conveniently centered on 0 (meaning no effect of the factor on y) and is shaped by the degrees of freedom. We then get a t-value for our beta estimate by dividing the beta by the standard error, and look at the probability of getting that t-value or one more extreme assuming the null hypothesis probability distribution is true. 

When we have random effects, getting the degrees of freedom to shape the t distribution is apparently problematic, so lme4 usually just gives us the t-value, and if it's well beyond 2, which is usually around the 2.5% cut-off point on each side of a normal distribution and most t distributions (i.e., we've cut off 5% of the distribution, p < 0.05), then we call it significantly different from 0.

Here we're given a 95% *credible interval* (not confidence interval because it's not the frequentist version). This interval tells you the range for the middle 95% of the sampled estimated beta coefficients are located. The estimate reported in the first column is the mean of this distribution; the est. error in the second column is the standard deviation of this distribution. If we want to run something *similar* to a null hypothesis test (but frequentist thinking here, hi reviewers), we want to know if our estimated beta statistically differs from 0. The simplest way to do this is to look to see if your 95% credible interval spans 0. If it does, then the probability of getting 0 as the estimated beta coefficient is reasonably high. 

**But critical!** This is a Bayesian framing of the question! We're currently asking: what's the probability of getting 0 (a hypothesis we're putting out there) assuming the probability distribution of estimates from our data/prior combo (i.e., the posterior probability distribution). The frequentist NHST approach asks: what's the probability of getting these estimates assuming an estimate of 0 is most probable (the null hypothesis)? In the Bayesian approach, if we see that 0 is contained within the 95% CI, then the probability of getting 0 or something in the opposite direction of our mean estimate is greater than 0.05. If 0 is not contained in the 95% CI, then the probability of getting 0 or something in the opposite direction of our mean estimate is less than 0.05 (and maybe even less than 0.025...)

I try to avoid using the term significant here as this is not a null hypothesis significance test, and people may otherwise be misled into thinking it is. I instead use terms like, there is compelling evidence for the direction and strength of an effect / there is compelling evidence that the direction and strength of this effect differs from 0. Also still taking suggestions on how to word this. 

## brms linear regression example 2

Let's now return to the model that gave us problems in lme4 -- the one with the maximal random effect structure. A lot of data is required in order to reliably estimate several random effects in non-Bayesian approaches, like in lme4. Using a Bayesian approach, however, can still allow for *estimation* of these effects (even though they may still have high variance). 

We'll once again assume the default prior and run the model just to show that brms can handle the more complex random effect structure.

```{r eval=F}
fit.brms.max <- brm(vot ~ phon + vdur + age + (1 + phon + vdur|spkr), buck)
```

### What are we doing, round 2
Now we wait again. This time, let's take a closer look at what's going on. We've got 4 chains starting and things like warm-up and iteration. What is this all? 

As mentioned in the last round of waiting, we are sampling beta coefficients (our parameters) from the posterior distribution. Why can't we just multiply the likelihood with the prior and divide by the evidence? Math and messy/impossible integrals. So, we have to approximate it by sampling the posterior distribution. I will do my best to explain. 

brms uses the No-U-Turn Sampling (NUTS) algorithm which is a variant of the Hamiltonian Monte Carlo algorithm. I'm not sure I can do this sampling algorithm justice yet, but I can try to explain the gist of sampling and Monte Carlo Markov Chains. 

It's weird to think of sampling from the posterior distribution when that's also what we're trying to figure out. How do we sample from something we don't know? In many cases, we start by making guesses (and sometimes informed guesses if we have a prior), and then try to improve those guesses according to our data (likelihood) and prior. I'll *attempt* to summarize the Metropolis-Hastings sampler, which is a relative of the Hamiltonian Monte Carlo. 

In Metropolis-Hastings, we have a *target distribution* and a *proposal distribution* that we're always choosing between. To start the chain of samples off, we start by proposing a best guess of what the parameters are by drawing random samples from our prior distribution. We compute the probability of the data given the parameters (likelihood) and multiply it by the probability of these parameters in our prior distribution. Hey, we have a posterior. On the first round, we have to accept those parameters because we have nothing to choose between. In subsequent rounds, we'll have some choice between new samples and old samples. 

In subsequent rounds, we draw another set of random samples from our newly created target distribution. We compute the probability of the data given these parameters (likelihood) and multiply it by the probability of these parameters in our prior distribution. This is our proposal distribution. We then have to decide whether or not to accept the proposal distribution. This is decided by a coin toss where the weight of the coin is determined by a ratio of the probabilities from the proposal and target distributions. We repeat this process for a preset number of iterations (usually several thousand). Hence, the chain. 

After all these rounds of sampling, we can take a look at the mess of samples we've drawn during our walk through the sample space. We should have hit some parameter values much more frequently than others. When working properly, these chains should fluctuate around a single parameter value. By accumulating a bunch of samples, we've created the posterior probability distribution: some values have high probability and some have low probability. We can then report our findings about the most probable estimates and the 95% CI. 

#### Iterations and Chains
You can adjust the number of iterations to go through in sampling, and even the number of chains that are used. The NUTS sampler happens to use parallel chains of sampling. The default number of iterations is 2000 and default number of chains is 4.

```{r eval=F}
fit.brms.max <- brm(vot ~ phon + vdur + age + (1 + phon + vdur|spkr), buck, iter = 3000, chains = 6)
```

#### Warm-up
We might also want to throw out the beginning rounds when we were really just guessing and finding our footing in the sample space. Don't worry, brms does this for you by default: this is the warm-up stage and those samples are thrown out. By default, the warm-up is set to half of the number of iterations. This can be changed using the warmup= option in the brm code. 

#### Transition probability
You may sometimes encounter an error of "There were x divergent transitions after warmup." In this case, that flip of the coin in the transition probability is still a little wonky, and the chain may be still be jumping all over the place. To correct for this, you can increase the parameter adapt_delta. Its default is 0.8, and when I encountered the error, I set it to 0.999. This slows the sampler down, but will minimize crazy jumps in the chain. It's recommended that adapt_delta be set between 0.8 and 1. Example code below.

```{r eval=F}
fit.brms.max <- brm(vot ~ phon + vdur + age + (1 + phon + vdur|spkr), buck, control = list(adapt_delta = 0.99))
```
 
#### Make it go faster! 
By default, R uses only one core of your computer to run analyses. Many standard computers, though, have more than one core. For example, my Macbook Pro with an Intel Core i7 Processor CPU has 4 cores. You can ask R to run the process in parallel on the cores it can detect, which will speed the process up. To do this, you can run this code before running brm. This will provide brm with the default option of core="mc.cores". 

```{r eval=F}
### run code on multiple cores
options(mc.cores=parallel::detectCores())
###
```

Or you can manually set this as follows: 

```{r eval=F}
fit.brms.max <- brm(vot ~ phon + vdur + age + (1 + phon + vdur|spkr), buck, control = list(adapt_delta = 0.99), core=4)
```

#### Plot the effects

Here's the very default code to plot the conditional effects. The packages tidyverse and bayesplot could also be helpful.

```{r eval=F}
conditional_effects(fit.brms.max)
```

# /s/-/S/ categorical perception

```{r message = FALSE}
#z <- read_csv("~/Desktop/zExposure.csv")
#z.orig <- z
```

This dataset comes from an experiment where participants were exposed to a speaker with either high or low COG /z/s and tested on /s/-/S/ categorization. The goal was to determine whether listeners generalized a speaker's COG from /z/ to the /s/-/S/ boundary. Take a look at the z dataset. It already includes predefined contrast columns. The goal here is to run a logistic mixed-effects regression using brms.

In this case, we want to predict the probability of responding /s/ (indicated by resp==1) based on the COG condition and the step of the /s/-/S/ continuum (high values correspond to more /s/ like, low values to more /S/ like). We'll also include a random intercept for subject and the exposure /z/ word they heard on that trial. Note that the main difference here from the linear regression is the family specification. Just like in lme4, you have to specify the family, but instead of calling it family=binomial(), it is family=bernoulli() in brms.  

```{r eval=F}
#fit.z.brmsx <- brm(I(resp==1) ~ Ncond + Nstep + (1 + Ncond + Nstep | subj) + (1 | word), family=bernoulli(), data=z, control = list(adapt_delta = 0.999))
```

```{r}
#summary(fit.z.brmsx)
```
